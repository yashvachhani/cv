{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out Paint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv2D, AtrousConvolution2D\n",
    "from keras.layers import Activation, Dense, Input, Conv2DTranspose, Dense, Flatten\n",
    "from keras.layers import ReLU, Dropout, Concatenate, BatchNormalization, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras_contrib.layers import InstanceNormalization\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "from dataloader import Data, TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/prepared_data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-43ed2878b83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/s6 project/computer_vision/Image-OutPainting-master/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.npy'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/prepared_data/train'"
     ]
    }
   ],
   "source": [
    "# Initialize dataloader\n",
    "data = Data()\n",
    "test_data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves Model in every N minutes\n",
    "TIME_INTERVALS = 2\n",
    "SHOW_SUMMARY = True\n",
    "\n",
    "INPUT_SHAPE = (256, 256, 3)\n",
    "EPOCHS = 500\n",
    "BATCH = 1\n",
    "\n",
    "# 25% i.e 64 width size will be mask from both side\n",
    "MASK_PERCENTAGE = .25\n",
    "\n",
    "EPSILON = 1e-9\n",
    "ALPHA = 0.0004\n",
    "\n",
    "CHECKPOINT = \"checkpoint/\"\n",
    "SAVED_IMAGES = \"saved_images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcrm_loss(y_true, y_pred):\n",
    "    return -tf.reduce_mean(tf.log(tf.maximum(y_true, EPSILON)) + tf.log(tf.maximum(1. - y_pred, EPSILON)))\n",
    "\n",
    "d_input_shape = (INPUT_SHAPE[0], int(INPUT_SHAPE[1] * (MASK_PERCENTAGE *2)), INPUT_SHAPE[2])\n",
    "d_dropout = 0.25\n",
    "DCRM_OPTIMIZER = Adam(0.0001, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_build_conv(layer_input, filter_size, kernel_size=4, strides=2, activation='leakyrelu', dropout_rate=d_dropout, norm=True):\n",
    "    c = Conv2D(filter_size, kernel_size=kernel_size, strides=strides, padding='same')(layer_input)\n",
    "    if activation == 'leakyrelu':\n",
    "        c = LeakyReLU(alpha=0.2)(c)\n",
    "    if dropout_rate:\n",
    "        c = Dropout(dropout_rate)(c)\n",
    "    if norm == 'inst':\n",
    "        c = InstanceNormalization()(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    d_input = Input(shape=d_input_shape)\n",
    "    d = d_build_conv(d_input, 32, 5,strides=2, norm=False)\n",
    "\n",
    "    d = d_build_conv(d, 64, 5, strides=2)\n",
    "    d = d_build_conv(d, 64, 5, strides=2)\n",
    "    d = d_build_conv(d, 128, 5, strides=2)\n",
    "    d = d_build_conv(d, 128, 5, strides=2)\n",
    "    \n",
    "    flat = Flatten()(d)\n",
    "    fc1 = Dense(1024, activation='relu')(flat)\n",
    "    d_output = Dense(1, activation='sigmoid')(fc1)\n",
    "    \n",
    "    return Model(d_input, d_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 64, 32)       2432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 64, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 64, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 32, 64)        51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 8, 128)        204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 4,967,169\n",
      "Trainable params: 4,967,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Discriminator initialization\n",
    "DCRM = build_discriminator()\n",
    "DCRM.compile(loss=dcrm_loss, optimizer=DCRM_OPTIMIZER)\n",
    "if SHOW_SUMMARY:\n",
    "    DCRM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss(y_true, y_pred):\n",
    "    G_MSE_loss = K.mean(K.square(y_pred - y_true))\n",
    "    return G_MSE_loss - ALPHA * tf.reduce_mean(tf.log(tf.maximum(y_pred, EPSILON)))\n",
    "\n",
    "g_input_shape = (INPUT_SHAPE[0], int(INPUT_SHAPE[1] * (MASK_PERCENTAGE *2)), INPUT_SHAPE[2])\n",
    "g_dropout = 0.25\n",
    "GEN_OPTIMIZER = Adam(0.001, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_build_conv(layer_input, filter_size, kernel_size=4, strides=2, activation='leakyrelu', dropout_rate=g_dropout, norm='inst', dilation=1):\n",
    "    c = AtrousConvolution2D(filter_size, kernel_size=kernel_size, strides=strides,atrous_rate=(dilation,dilation), padding='same')(layer_input)\n",
    "    if activation == 'leakyrelu':\n",
    "        c = ReLU()(c)\n",
    "    if dropout_rate:\n",
    "        c = Dropout(dropout_rate)(c)\n",
    "    if norm == 'inst':\n",
    "        c = InstanceNormalization()(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def g_build_deconv(layer_input, filter_size, kernel_size=3, strides=2, activation='relu', dropout=0):\n",
    "    d = Conv2DTranspose(filter_size, kernel_size=kernel_size, strides=strides, padding='same')(layer_input)\n",
    "    if activation == 'relu':\n",
    "        d = ReLU()(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    g_input = Input(shape=g_input_shape)\n",
    "    \n",
    "    g1 = g_build_conv(g_input, 64, 5, strides=1)\n",
    "    g2 = g_build_conv(g1, 128, 4, strides=2)\n",
    "    g3 = g_build_conv(g2, 256, 4, strides=2)\n",
    "\n",
    "    g4 = g_build_conv(g3, 512, 4, strides=1)\n",
    "    g5 = g_build_conv(g4, 512, 4, strides=1)\n",
    "    \n",
    "    g6 = g_build_conv(g5, 512, 4, strides=1, dilation=2)\n",
    "    g7 = g_build_conv(g6, 512, 4, strides=1, dilation=4)\n",
    "    g8 = g_build_conv(g7, 512, 4, strides=1, dilation=8)\n",
    "    g9 = g_build_conv(g8, 512, 4, strides=1, dilation=16)\n",
    "    \n",
    "    g10 = g_build_conv(g9, 512, 4, strides=1)\n",
    "    g11 = g_build_conv(g10, 512, 4, strides=1)\n",
    "    \n",
    "    g12 = g_build_deconv(g11, 256, 4, strides=2)\n",
    "    g13 = g_build_deconv(g12, 128, 4, strides=2)\n",
    "    \n",
    "    g14 = g_build_conv(g13, 128, 4, strides=1)\n",
    "    g15 = g_build_conv(g14, 64, 4, strides=1)\n",
    "    \n",
    "    g_output = AtrousConvolution2D(3, kernel_size=4, strides=(1,1), activation='tanh',padding='same', atrous_rate=(1,1))(g15)\n",
    "    \n",
    "    return Model(g_input, g_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/opt/anaconda3/lib/python3.7/site-packages/keras/legacy/layers.py:304: UserWarning: The `AtrousConvolution2D` layer  has been deprecated. Use instead the `Conv2D` layer with the `dilation_rate` argument.\n",
      "  warnings.warn('The `AtrousConvolution2D` layer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 256, 128, 64)      4864      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 256, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_1 (In (None, 256, 128, 64)      2         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 64, 128)      131200    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 128, 64, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128, 64, 128)      0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_2 (In (None, 128, 64, 128)      2         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_3 (In (None, 64, 32, 256)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 32, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_4 (In (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_5 (In (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_6 (In (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_7 (In (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_8 (In (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_9 (In (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_10 (I (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 32, 512)       4194816   \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_11 (I (None, 64, 32, 512)       2         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 128, 64, 256)      2097408   \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 128, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 256, 128, 128)     524416    \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 256, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 256, 128, 128)     262272    \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 256, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_12 (I (None, 256, 128, 128)     2         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 256, 128, 64)      131136    \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 256, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "instance_normalization_13 (I (None, 256, 128, 64)      2         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 256, 128, 3)       3075      \n",
      "=================================================================\n",
      "Total params: 35,140,317\n",
      "Trainable params: 35,140,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generator Initialization\n",
    "GEN = build_generator()\n",
    "GEN.compile(loss=gen_loss, optimizer=GEN_OPTIMIZER)\n",
    "if SHOW_SUMMARY:\n",
    "    GEN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = Input(shape=g_input_shape)\n",
    "DCRM.trainable = False\n",
    "GENERATED_IMAGE = GEN(IMAGE)\n",
    "CONF_GENERATED_IMAGE = DCRM(GENERATED_IMAGE)\n",
    "\n",
    "COMBINED = Model(IMAGE, [CONF_GENERATED_IMAGE, GENERATED_IMAGE])\n",
    "COMBINED.compile(loss=['mse', 'mse'], optimizer=GEN_OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking and De-Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_width(img):\n",
    "    image = img.copy()\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    new_width = int(width * MASK_PERCENTAGE)\n",
    "    mask = np.ones([height, new_width, 3])\n",
    "    missing_x = img[:, :new_width]\n",
    "    missing_y = img[:, width - new_width:]\n",
    "    missing_part = np.concatenate((missing_x, missing_y), axis=1)\n",
    "    image = image[:, :width - new_width]\n",
    "    image = image[:, new_width:]\n",
    "    return image, missing_part\n",
    "\n",
    "\n",
    "def get_masked_images(images):\n",
    "    mask_images = []\n",
    "    missing_images = []\n",
    "    for image in images:\n",
    "        mask_image, missing_image = mask_width(image)\n",
    "        mask_images.append(mask_image)\n",
    "        missing_images.append(missing_image)\n",
    "    return np.array(mask_images), np.array(missing_images)\n",
    "\n",
    "\n",
    "def get_demask_images(original_images, generated_images):\n",
    "    demask_images = []\n",
    "    for o_image, g_image in zip(original_images, generated_images):\n",
    "        width = g_image.shape[1] // 2\n",
    "        x_image = g_image[:, :width]\n",
    "        y_image = g_image[:, width:]\n",
    "        o_image = np.concatenate((x_image,o_image, y_image), axis=1)\n",
    "        demask_images.append(o_image)\n",
    "    return np.asarray(demask_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f914f6a6a975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Masking, Demasking example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Note: IPython display gives false colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# a will be the input and b will be the output for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Masking, Demasking example\n",
    "# Note: IPython display gives false colors.\n",
    "x = data.get_data(1)\n",
    "\n",
    "# a will be the input and b will be the output for the model\n",
    "a, b = get_masked_images(x)\n",
    "border = np.ones([x[0].shape[0], 10, 3]).astype(np.uint8)\n",
    "print('After masking')\n",
    "print('\\tOriginal Image\\t\\t\\t a \\t\\t b')\n",
    "image = np.concatenate((border, x[0],border,a[0],border, b[0], border), axis=1)\n",
    "IPython.display.display(PIL.Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "print(\"After desmasking: 'b/2' + a + 'b/2' \")\n",
    "c = get_demask_images(a,b)\n",
    "IPython.display.display(PIL.Image.fromarray(cv2.cvtColor(c[0], cv2.COLOR_BGR2RGB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "1. Save Model\n",
    "2. Load Model\n",
    "3. Save Image\n",
    "4. Save Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    global DCRM, GEN\n",
    "    models = [DCRM, GEN]\n",
    "    model_names = ['DCRM','GEN']\n",
    "\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        model_path =  CHECKPOINT + \"%s.json\" % model_name\n",
    "        weights_path = CHECKPOINT + \"/%s.hdf5\" % model_name\n",
    "        options = {\"file_arch\": model_path, \n",
    "                    \"file_weight\": weights_path}\n",
    "        json_string = model.to_json()\n",
    "        open(options['file_arch'], 'w').write(json_string)\n",
    "        model.save_weights(options['file_weight'])\n",
    "    print(\"Saved Model\")\n",
    "    \n",
    "    \n",
    "def load_model():\n",
    "    # Checking if all the model exists\n",
    "    model_names = ['DCRM', 'GEN']\n",
    "    files = os.listdir(CHECKPOINT)\n",
    "    for model_name in model_names:\n",
    "        if model_name+\".json\" not in files or\\\n",
    "           model_name+\".hdf5\" not in files:\n",
    "            print(\"Models not Found\")\n",
    "            return\n",
    "    global DCRM, GEN, COMBINED, IMAGE, GENERATED_IMAGE, CONF_GENERATED_IMAGE\n",
    "    \n",
    "    # load DCRM Model\n",
    "    model_path = CHECKPOINT + \"%s.json\" % 'DCRM'\n",
    "    weight_path = CHECKPOINT + \"%s.hdf5\" % 'DCRM'\n",
    "    with open(model_path, 'r') as f:\n",
    "        DCRM = model_from_json(f.read())\n",
    "    DCRM.load_weights(weight_path)\n",
    "    DCRM.compile(loss=dcrm_loss, optimizer=DCRM_OPTIMIZER)\n",
    "    \n",
    "    #load GEN Model\n",
    "    model_path = CHECKPOINT + \"%s.json\" % 'GEN'\n",
    "    weight_path = CHECKPOINT + \"%s.hdf5\" % 'GEN'\n",
    "    with open(model_path, 'r') as f:\n",
    "         GEN = model_from_json(f.read(), custom_objects={'InstanceNormalization': InstanceNormalization()})\n",
    "    GEN.load_weights(weight_path)\n",
    "    \n",
    "    # Combined Model\n",
    "    DCRM.trainable = False\n",
    "    IMAGE = Input(shape=g_input_shape)\n",
    "    GENERATED_IMAGE = GEN(IMAGE)\n",
    "    CONF_GENERATED_IMAGE = DCRM(GENERATED_IMAGE)\n",
    "\n",
    "    COMBINED = Model(IMAGE, [CONF_GENERATED_IMAGE, GENERATED_IMAGE])\n",
    "    COMBINED.compile(loss=['mse', 'mse'], optimizer=GEN_OPTIMIZER)\n",
    "    \n",
    "    print(\"loaded model\")\n",
    "    \n",
    "    \n",
    "def save_image(epoch, steps):\n",
    "    train_image = test_data.get_data(1)\n",
    "    if train_image is None:\n",
    "        train_image = test_data.get_data(1)\n",
    "        \n",
    "    test_image = data.get_data(1)\n",
    "    if test_image is None:\n",
    "        test_image = test_data.get_data(1)\n",
    "    \n",
    "    for nc, original in enumerate([train_image, test_image]):\n",
    "        if nc:\n",
    "            print(\"Predicting with train image\")\n",
    "        else:\n",
    "            print(\"Predicting with test image\")\n",
    "            \n",
    "        mask_image_original , missing_image = get_masked_images(original)\n",
    "        mask_image = mask_image_original.copy()\n",
    "        mask_image = mask_image / 127.5 - 1\n",
    "        missing_image = missing_image / 127.5 - 1\n",
    "        gen_missing = GEN.predict(mask_image)\n",
    "        gen_missing = (gen_missing + 1) * 127.5\n",
    "        gen_missing = gen_missing.astype(np.uint8)\n",
    "        demask_image = get_demask_images(mask_image_original, gen_missing)\n",
    "\n",
    "        mask_image = (mask_image + 1) * 127.5\n",
    "        mask_image = mask_image.astype(np.uint8)\n",
    "\n",
    "        border = np.ones([original[0].shape[0], 10, 3]).astype(np.uint8)\n",
    "\n",
    "        file_name = str(epoch) + \"_\" + str(steps) + \".jpg\"\n",
    "        final_image = np.concatenate((border, original[0],border,mask_image_original[0],border, demask_image[0], border), axis=1)\n",
    "        if not nc:\n",
    "            cv2.imwrite(os.path.join(SAVED_IMAGES, file_name), final_image)\n",
    "        final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB)\n",
    "        print(\"\\t1.Original image \\t 2.Input \\t\\t 3. Output\")\n",
    "        IPython.display.display(PIL.Image.fromarray(final_image))\n",
    "        print(\"image saved\")\n",
    "\n",
    "\n",
    "def save_log(log):\n",
    "    with open('log.txt', 'a') as f:\n",
    "        f.write(\"%s\\n\"%log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start_time = datetime.now()\n",
    "    saved_time = start_time\n",
    "    \n",
    "    global MIN_D_LOSS, MIN_G_LOSS, CURRENT_D_LOSS, CURRENT_G_LOSS\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        steps = 1\n",
    "        test = None\n",
    "        while True:\n",
    "            original = data.get_data(BATCH)\n",
    "            if original is None:\n",
    "                break\n",
    "            batch_size = original.shape[0]\n",
    "\n",
    "            mask_image, missing_image = get_masked_images(original)\n",
    "            mask_image = mask_image / 127.5 - 1\n",
    "            missing_image = missing_image / 127.5 - 1\n",
    "\n",
    "            # Train Discriminator\n",
    "            gen_missing = GEN.predict(mask_image)\n",
    "\n",
    "            real = np.ones([batch_size, 1])\n",
    "            fake = np.zeros([batch_size, 1])\n",
    "            \n",
    "            d_loss_original = DCRM.train_on_batch(missing_image, real)\n",
    "            d_loss_mask = DCRM.train_on_batch(gen_missing, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_original, d_loss_mask)\n",
    "\n",
    "            # Train Generator\n",
    "            for i in range(2):\n",
    "                g_loss = COMBINED.train_on_batch(mask_image, [real, missing_image])\n",
    "                    \n",
    "            log = \"epoch: %d, steps: %d, DIS loss: %s, GEN loss: %s, Identity loss: %s\" \\\n",
    "                                            %(epoch, steps, str(d_loss), str(g_loss[0]), str(g_loss[2]))\n",
    "            print(log)\n",
    "            save_log(log)\n",
    "            steps += 1\n",
    "            \n",
    "            # Save model if time taken > TIME_INTERVALS\n",
    "            current_time = datetime.now()\n",
    "            difference_time = current_time - saved_time\n",
    "            if difference_time.seconds >= (TIME_INTERVALS * 60):\n",
    "                save_model()\n",
    "                save_image(epoch, steps)\n",
    "                saved_time = current_time\n",
    "        clear_output()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive paint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_paint(image, factor=3):\n",
    "    final_image = None\n",
    "    gen_missing = None\n",
    "    for i in range(factor):\n",
    "        demask_image = None\n",
    "        if i == 0:\n",
    "            x, y = get_masked_images([image])\n",
    "            gen_missing = GEN.predict(x)\n",
    "            final_image = get_demask_images(x, gen_missing)[0]\n",
    "        else:\n",
    "            gen_missing = GEN.predict(gen_missing)\n",
    "            final_image = get_demask_images([final_image], gen_missing)[0]\n",
    "    return final_image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5269e8bbad11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecursive_paint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m127.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "images = data.get_data(1)\n",
    "for i, image in enumerate(images):\n",
    "    image = image / 127.5 - 1\n",
    "    image = recursive_paint(image)\n",
    "    image = (image + 1) * 127.5\n",
    "    image = image.astype(np.uint8)\n",
    "    path = 'recursive/'+str(i)+'.jpg'\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    IPython.display.display(PIL.Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:4045: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-df2ac4de4f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcropped_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m193\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcropped_image\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:4045: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "#url = 'https://upload.wikimedia.org/wikipedia/commons/3/33/A_beach_in_Maldives.jpg'\n",
    "url = '/Users/yash/Downloads/t5'\n",
    "\n",
    "#file_name = os.path.basename(url)\n",
    "#import urllib.request\n",
    "#_ = urllib.request.urlretrieve(url, file_name)\n",
    "#print(\"Downloaded image\")\n",
    "\n",
    "image = cv2.imread(file_name)\n",
    "image = cv2.resize(image, (256,256))\n",
    "cropped_image = image[:, 65:193]\n",
    "input_image = cropped_image / 127.5 - 1\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "print(input_image.shape)\n",
    "predicted_image = GEN.predict(input_image)\n",
    "predicted_image = get_demask_images(input_image, predicted_image)[0]\n",
    "predicted_image = (predicted_image + 1) * 127.5\n",
    "predicted_image = predicted_image.astype(np.uint8)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "predicted_image = cv2.cvtColor(predicted_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print('original image')\n",
    "IPython.display.display(PIL.Image.fromarray(image))\n",
    "print('predicted image')\n",
    "IPython.display.display(PIL.Image.fromarray(predicted_image))\n",
    "\n",
    "os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
